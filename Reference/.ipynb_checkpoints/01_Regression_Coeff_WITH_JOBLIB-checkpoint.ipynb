{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5fb67b",
   "metadata": {},
   "source": [
    "# Regression Coefficients - Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c4f43",
   "metadata": {},
   "source": [
    "## Lesson Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b236d8",
   "metadata": {},
   "source": [
    "By the end of this lesson, students will be able to:\n",
    "- Use scikit-learn v1.1's simplified toolkit.\n",
    "- Extract and visualize coefficients from sklearn regression model. \n",
    "- Control panda's display options to facilitate interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e5567",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5efe1",
   "metadata": {},
   "source": [
    "- At the end of last stack, we dove deep into linear regression models and their assumptions. We introduced a new package called statsmodels, which produced a Linear Regression model using \"Ordinary-Least-Squared (OLS)\". \n",
    "- The model included a robust statistical summary that was incredibly informative as we critically diagnosed our regression model and if we met the assumptions of linear regression.\n",
    "- This stack, we will be focusing on extracting insights from our models: both by examining parameters/aspects of the model itself, like the coefficients it calculated, but also by applying some additional tools and packages specifically designed to explain models. \n",
    "\n",
    "- Most of these tools are compatible with the scikit-learn ecosystem but are not yet available for statsmodels.\n",
    "\n",
    "Since we are not focusing on regression diagnostics this week, we will shift back to using scikit-learn models. Scikit-learn recently released version 1.1.1, which added several helpful tools that will simplify our workflow. \n",
    "\n",
    "Let's review some of these key updates as we rebuild our housing regression model from week 16.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0262e",
   "metadata": {},
   "source": [
    "# Confirming Package Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadae440",
   "metadata": {},
   "source": [
    "- All packages have a version number that indicates which iteration of the package is currently being used.\n",
    "    - If you import an entire package, you can use the special method `package.__version__` (replace package with the name of the package you want to check).\n",
    "- The reason this is important is that as of the writing of this stack, Google Colab is still using a version of python that is too old to support the newest scikit-learn.\n",
    "    - You can check which version of python you are using by running the following command in a jupyter notebook:\n",
    "        - `!python --version`\n",
    "        - Note: if you remove the `!`, you can run this command in your terminal.\n",
    "\n",
    "- If you run the following code on Google Colab and on your local computer, you can compare the version numbers. \n",
    "        \n",
    "<img src=\"colab_versions.png\" width=400px>\n",
    "\n",
    "- Now, run the following block of code in a jupyter notebook on your local machine to confirm that you have Python 3.8.13 and sklearn v1.1.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following command on your local computer to \n",
    "import sklearn\n",
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a308cbc",
   "metadata": {},
   "source": [
    "\n",
    ">- If you have a Python 3.7 or an earlier version of scikit-learn, please revisit the \"`<Insert the name of the \"week\" of content on the LP for installation>`\". \n",
    "    - See the \"`Updating Your Dojo-Env Lesson` [Note to Brenda: does not exist yet - see 1:1 doc for question on handling multiple envs] for how to remove your current dojo-env and replace it with the new one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f7371d",
   "metadata": {},
   "source": [
    "# Extracting Coefficients from LinearRegression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd36035",
   "metadata": {},
   "source": [
    "## Highlighted Changes  - scikit-learn v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee56abb",
   "metadata": {},
   "source": [
    "- The single biggest change in the updated sklearn is a fully-functional `.get_feature_names_out()` method in the `ColumnTransformer`.\n",
    "    - This will make it MUCH easier for us to extract our transformed data as dataframes and to match up the feature names to our models' coefficients.\n",
    "- There are some additional updates that are not pertinent to this stack, but if you are curious, you can find the [details on the new release here](https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_1_1_0.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce77db0",
   "metadata": {},
   "source": [
    "## New and Improved `ColumnTransformer` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Customization Options\n",
    "plt.style.use(['fivethirtyeight','seaborn-talk'])\n",
    "mpl.rcParams['figure.facecolor']='white'\n",
    "\n",
    "## additional required imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer, make_column_selector, ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a649472",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the King's County housing dataset and display the head and info\n",
    "df = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vSEZQEzxja7Hmj5tr5nc52QqBvFQdCAGb52e1FRK1PDT2_TQrS6rY_TR9tjZjKaMbCy1m5217sVmI5q/pub?output=csv\")\n",
    "\n",
    "## Dropping some features for time\n",
    "df = df.drop(columns=['date'])\n",
    "display(df.head(),df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the house ids the index\n",
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Treating zipcode as a category\n",
    "df['zipcode'] = df['zipcode'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e70b91",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adbf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make x and y variables\n",
    "y = df['price'].copy()\n",
    "X = df.drop(columns=['price']).copy()\n",
    "\n",
    "## train-test-split with random state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=321)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1d516",
   "metadata": {},
   "source": [
    "### Preprocessing + ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make categorical selector and verifying it works \n",
    "cat_sel = make_column_selector(dtype_include='object')\n",
    "cat_sel(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51815cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make numeric selector and verifying it works \n",
    "num_sel = make_column_selector(dtype_include='number')\n",
    "num_sel(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make pipelines for categorical vs numeric data\n",
    "cat_pipe = make_pipeline(SimpleImputer(strategy='constant',\n",
    "                                       fill_value='MISSING'),\n",
    "                         OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "\n",
    "num_pipe = make_pipeline(SimpleImputer(strategy='mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c715d73",
   "metadata": {},
   "source": [
    "> Nothing we have done yet should be new code. The changes we will make will be when we create our ColumnTransformer with `make_column_transformer`.\n",
    "- From now on, you should add `verbose_feature_names_out=False` to `make_column_transformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the preprocessing column transformer\n",
    "preprocessor = make_column_transformer((num_pipe, num_sel),\n",
    "                                       (cat_pipe,cat_sel),\n",
    "                                      verbose_feature_names_out=False)\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE LATER - just demoing using classes directly\n",
    "preprocessor_class = ColumnTransformer([('num',num_pipe, num_sel),\n",
    "                                       ('cat',cat_pipe,cat_sel)])\n",
    "preprocessor_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54784c59",
   "metadata": {},
   "source": [
    ">- In order to extract the feature names from the preprocessor, we first have to fit it on the data.\n",
    "- Next, we can use the `preprocessor.get_feature_names_out()` method and save the output as something like \"feature_names\" or \"final_features\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408c2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit column transformer and run get_feature_names_out\n",
    "preprocessor.fit(X_train)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f8c53",
   "metadata": {},
   "source": [
    "- Notice how we were able to get the complete list of feature names, including the One Hot Encoded features with their proper \"zipcode\" prefix. \n",
    "- Quick note: if you forgot to add `verbose_feature_names_out` when you made your preprocessor, you would get something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make the preprocessing column transformer\n",
    "preprocessor_oops = make_column_transformer((num_pipe, num_sel),\n",
    "                                       (cat_pipe,cat_sel)\n",
    "                                           ) # forgot verbose_feature_names_out=False\n",
    "## fit column transformer and run get_feature_names_out\n",
    "preprocessor_oops.fit(X_train)\n",
    "feature_names_oops = preprocessor_oops.get_feature_names_out()\n",
    "feature_names_oops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456f6b8",
   "metadata": {},
   "source": [
    "### Remaking Our X_train and X_test as DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248d3a0",
   "metadata": {},
   "source": [
    "- Now that we have our list of feature names, we can very easily transform out X_train and X_test into preprocessed dataframes. \n",
    "- We can immediately turn the output of our preprocessor into a dataframe and do not need to save it as a separate variable first.\n",
    "    - Therefore, in our pd.DataFrame, we will provide the `preprocessor.transform(X_train)` as the first argument, followed by `columns=feature_names` (the list we extracted from our precprocessor)\n",
    "    - Pro Tip: you can also use the same index as your X_train or X_test variable, if you want to match up one of the transformed rows with the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed87f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(preprocessor.transform(X_train), \n",
    "                          columns = feature_names, index = X_train.index)\n",
    "X_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = pd.DataFrame(preprocessor.transform(X_test), \n",
    "                          columns = feature_names, index = X_test.index)\n",
    "X_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09185f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm the first 3 rows index in y_test matches X_test_df\n",
    "y_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc717b4",
   "metadata": {},
   "source": [
    "- Notice that we cannot see all of our features after OneHotEncoding. Pandas truncates the display in the middle and displays `...` instead. \n",
    "- We can get around this by changing the settings in Pandas using `pd.set_option`\n",
    "    - In this case, we want to change the `max_columns` to be a number larger than our number of final features. Since we have 87 features, setting the `max_columns` to 100 would be sufficient.\n",
    "- For more information on pandas options, see their [documentation on Options and Settings](https://pandas.pydata.org/docs/user_guide/options.html)\n",
    "- Final note: in your project notebooks, you should add this function to the top of your notebook right after your imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b910ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using pd.set_option to display more columns\n",
    "pd.set_option('display.max_columns',100)\n",
    "X_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231b8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aef5acc",
   "metadata": {},
   "source": [
    "## Extracting Coefficients and Intercept from Scikit-Learn Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6febc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## fitting a linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_df, y_train)\n",
    "print(f'Training R^2: {lin_reg.score(X_train_df, y_train):.3f}')\n",
    "print(f'Test R^2: {lin_reg.score(X_test_df, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1873313",
   "metadata": {},
   "source": [
    "- For scikit-learn Linear Regressions, we can find the coefficients for the features that were included in our X-data under the `.coef_` attribute. \n",
    "-  the `.coef_` is a numpy matrix that should have the same number of values as the # of columns in X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009b996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the number of coeffs matches the # of feature names\n",
    "print(len(lin_reg.coef_))\n",
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c604cfe",
   "metadata": {},
   "source": [
    "> Note: if for some reason the length of your coef_ is 1, you should add the `.flatten()` method to convert the  coef_ into a simple 1-D array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3c4af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T18:14:22.806525Z",
     "start_time": "2022-06-28T18:14:22.801222Z"
    }
   },
   "source": [
    "### Saving the coefficients as a pandas Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b8f3ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T18:06:01.097880Z",
     "start_time": "2022-06-28T18:06:01.088310Z"
    }
   },
   "source": [
    "- We can immediately turn the the models' .coef_ into a pd.Series, as well.\n",
    "    - Therefore, in our pd.Series, we will provide the `lin_reg.coef_` as the first argument, followed by `index=feature_names` (pandas Series are 1D and do not have columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475c439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the coefficients\n",
    "coeffs = pd.Series(lin_reg.coef_, index= feature_names)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325a5235",
   "metadata": {},
   "source": [
    "- The constant/intercept is not included in the .ceof_ attribute (if we used the default settings for LinearRegression which sets fit_intercept = True)\n",
    "- The intercept is stored in the `.intercept_` attribute \n",
    "- We can add this as a new value to our coeffs series.\n",
    "- Note: it is up to you what you name your intercept/constant. If you wanted to keep the naming convention of statsmodels, you could use \"const\" or just \"intercept\" for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b8b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .loc to add the intercept to the series\n",
    "coeffs.loc['intercept'] = lin_reg.intercept_\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91674c3d",
   "metadata": {},
   "source": [
    "### Displaying the Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b38cf",
   "metadata": {},
   "source": [
    "- Just like we increased the number of columns displayed by pandas, we can also increase the number of rows displayed by pandas.\n",
    "- CAUTION: DO NOT SET THE MAX ROWS TO 0!! If you try to display a dataframe that has 1,000,000 it will try to display ALL 1,000,000 rows and will crash your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431036c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',100)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55234237",
   "metadata": {},
   "source": [
    "### Suppressing Scientific Notation in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37641ee",
   "metadata": {},
   "source": [
    "> We can ALSO use panda's options to change how it display numeric values.\n",
    "- if we want to add a `,` separator for thousands and round to 2 decimal places, we would use the format code \",.2f\". \n",
    "- In order for Pandas to use this, we will have to use an f-string with a lambda x. (X represent any numeric value being displayed by pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: f\"{x:,.2f}\")\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7949d",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae9418",
   "metadata": {},
   "source": [
    "### Recap - Sklearn v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1523e47",
   "metadata": {},
   "source": [
    "- We added the argument `verbose_feature_names_out=False` to `make_column_transformer`, which let us extract our feature names (after fitting the preprocessor) using `.get_feature_names_out()`\n",
    "\n",
    "- We then used this list of features when reconstruction our transformed X_train and X_test as dataframes and when extracting coefficients from our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f15d6fe",
   "metadata": {},
   "source": [
    "### Recap - Pandas Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a9c9a",
   "metadata": {},
   "source": [
    "- We used the following options in our notebook. Ideally, we should group these together and move them to the top of our notebook, immediately after our imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd4f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reviewing the options used\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.float_format', lambda x: f\"{x:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec940bf",
   "metadata": {},
   "source": [
    "# 📌 BOOKMARK: End Lesson?\n",
    "- I can see either ending the lesson here and picking up with these coefficients in our next lesson, where we focus on visualizing the coefficients, examining some features more closely (e.g. bedrooms) and following up with scaling vs not-scaling coefficients.\n",
    "- OR including the visualization in this lesson and pickup with Scaling vs Not-Scaling and Intercept vs no-intercept.\n",
    "\n",
    "- ALSO: I am using the following code for myself, but it may be good to include here. It was at the end of week 16 and many students probably didn't see it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990f5727",
   "metadata": {},
   "source": [
    "## Saving Our Model for Later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b3079e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T18:46:41.149363Z",
     "start_time": "2022-06-28T18:46:41.138517Z"
    }
   },
   "source": [
    "- We will continue to explore the results from this model in the next lesson. \n",
    "    - We will create a dictionary of variables we want to export to use in a subsequent notebook/analysis. It will include\n",
    "    \n",
    "- While pickle is a common tool used for this, the joblib package has become increasing popular. Scikit-learn now promotes joblib files as the best way to save models. \n",
    "\n",
    "    - Here is the [section of the User Guide on \"Serializing models\"](https://scikit-learn.org/stable/modules/model_persistence.html#python-specific-serialization) where they demonstrate using joblib.\n",
    "    \n",
    "    \n",
    "- To make it easy to remember which variable was which, we will save the data and model into a dictionary first.\n",
    "    - We will save our:\n",
    "        - Outlier removed training and test data\n",
    "        - Our preprocessing column transformer\n",
    "        - The scaler we used to transform price when looking for outliers \n",
    "        - Our OLS results that contain the .summary()\n",
    "    - Then we will save the dictionary to a joblib file.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving variables for next lesson/notebook\n",
    "import joblib\n",
    "\n",
    "export = {'X_train':X_train_df,\n",
    "         'y_train': y_train,\n",
    "         'X_test':X_test_df, \n",
    "         \"y_test\": y_test,\n",
    "         'preprocessor':preprocessor,\n",
    "         'model':lin_reg,\n",
    "         'coeffs':coeffs}\n",
    "\n",
    "joblib.dump(export, 'lesson01_vars.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99320be4",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b52f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding folder above to path\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "## Load stack_functions with autoreload turned on\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from CODE import stack_functions as sf\n",
    "\n",
    "def show_code(function):\n",
    "    import inspect \n",
    "    from IPython.display import display,Markdown, display_markdown\n",
    "    code = inspect.getsource(function)\n",
    "    md_txt = f\"```python\\n{code}\\n```\"\n",
    "    return display(Markdown(md_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a8349",
   "metadata": {},
   "source": [
    "# 📌 BOOKMARK 2: Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71997b4d",
   "metadata": {},
   "source": [
    "## Outlier/Anomaly Detection & Removal (with sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf5b58",
   "metadata": {},
   "source": [
    "- User Guide:\n",
    "    - https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "- Models:\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698949ce",
   "metadata": {},
   "source": [
    "### Model WITH Outliers Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7063b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "## ORIGINAL LINREG\n",
    "## fitting a linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_df, y_train)\n",
    "print(f'Training R^2: {lin_reg.score(X_train_df, y_train):.3f}')\n",
    "print(f'Test R^2: {lin_reg.score(X_test_df, y_test):.3f}')\n",
    "sf.evaluate_ols(lin_reg, X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17aeb50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e8a1b",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = IsolationForest()\n",
    "detector.fit(X_train_df, y_train)\n",
    "\n",
    "output_train = detector.predict(X_train_df)\n",
    "# output_test = detector.predict(X_test_df)\n",
    "\n",
    "display(pd.Series(output_train, name='Train Outliers').value_counts())#,\n",
    "# pd.Series(output_test, name='Test Outliers').value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11427511",
   "metadata": {},
   "source": [
    "> Trying isolation forest with y vars added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfeebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train_df, y_train],axis=1)\n",
    "df_test = pd.concat([X_test_df, y_test],axis=1)\n",
    "display(df_train.head(3), df_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = IsolationForest()\n",
    "detector.fit(df_train)\n",
    "\n",
    "output_train = detector.predict(df_train)\n",
    "# output_test = detector.predict(df_test)\n",
    "\n",
    "display(pd.Series(output_train, name='Train Outliers').value_counts())#,\n",
    "# pd.Series(output_test, name='Test Outliers').value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c29754",
   "metadata": {},
   "source": [
    "### Local Outlier Factor [UserGuide](https://scikit-learn.org/stable/modules/outlier_detection.html#local-outlier-factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = LocalOutlierFactor()\n",
    "detector.fit_predict(X_train_df)#, y_train)\n",
    "\n",
    "output_train = pd.Series(detector.fit_predict(X_train_df), \n",
    "                         index=X_train_df.index,\n",
    "                        name='Train Outliers')\n",
    "# output_test = pd.Series(detector.predict(X_test_df), index=X_test_df.index,\n",
    "#                        name='Test Outliers')\n",
    "\n",
    "display(output_train.value_counts())#, output_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45490cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing outliers in training data\n",
    "X_train_df[ output_train ==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keeping only good rows\n",
    "X_train_cln = X_train_df.loc[output_train==1]\n",
    "y_train_cln = y_train.loc[ output_train==1]\n",
    "X_train_cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c60874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_cln = X_test_df.loc[output_test==1]\n",
    "# y_test_cln = y_test.loc[output_test==1]\n",
    "# X_test_cln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bff534",
   "metadata": {},
   "source": [
    "#### Model wtih LocalOutlierFactor outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f60fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fitting a linear regression model\n",
    "lin_reg_cln = LinearRegression()\n",
    "lin_reg_cln.fit(X_train_cln, y_train_cln)\n",
    "print(f'Training R^2: {lin_reg_cln.score(X_train_cln, y_train_cln):.3f}')\n",
    "print(f'Test R^2: {lin_reg_cln.score(X_test_df, y_test):.3f}')\n",
    "sf.evaluate_ols(lin_reg_cln, X_train_cln, y_train_cln)\n",
    "sf.evaluate_ols(lin_reg_cln, X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169abab7",
   "metadata": {},
   "source": [
    "### Removing Outliers, including target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fda2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train_df, y_train],axis=1)\n",
    "# df_test = pd.concat([X_test_df, y_test],axis=1)\n",
    "display(df_train.head(3))#, df_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying concat vars\n",
    "detector = LocalOutlierFactor()#novelty=True)\n",
    "output_train = pd.Series(detector.fit_predict(X_train_df), \n",
    "                         index=X_train_df.index,\n",
    "                        name='Train Outliers')\n",
    "\n",
    "display(output_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b035ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keeping only good rows\n",
    "X_train_cln = X_train_df.loc[output_train==1]\n",
    "y_train_cln = y_train.loc[ output_train==1]\n",
    "# X_test_cln = X_test_df.loc[output_test==1]\n",
    "# y_test_cln = y_test.loc[output_test==1]\n",
    "# X_test_cln\n",
    "X_train_cln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a2af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fitting a linear regression model\n",
    "lin_reg_cln = LinearRegression()\n",
    "lin_reg_cln.fit(X_train_cln, y_train_cln)\n",
    "print(f'Training R^2: {lin_reg_cln.score(X_train_cln, y_train_cln):.3f}')\n",
    "print(f'Test R^2: {lin_reg_cln.score(X_test_df, y_test):.3f}')\n",
    "sf.evaluate_ols(lin_reg_cln, X_train_cln, y_train_cln)\n",
    "sf.evaluate_ols(lin_reg_cln, X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13396a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ORIGINAL LINREG\n",
    "## fitting a linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_df, y_train)\n",
    "print(f'Training R^2: {lin_reg.score(X_train_df, y_train):.3f}')\n",
    "print(f'Test R^2: {lin_reg.score(X_test_df, y_test):.3f}')\n",
    "sf.evaluate_ols(lin_reg, X_train_df, y_train)\n",
    "sf.evaluate_ols(lin_reg, X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203887fb",
   "metadata": {},
   "source": [
    "### Comparing with IQR Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_code(sf.remove_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train_df, y_train],axis=1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fcecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cols = [c for c in df_train.columns if not c.startswith('zipcode')]\n",
    "outlier_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cln = sf.remove_outliers(df_train, subset=outlier_cols)\n",
    "\n",
    "X_train_iqr = df_train_cln.drop(columns='price')\n",
    "y_train_iqr = df_train_cln['price']\n",
    "X_train_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82854a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fitting a linear regression model\n",
    "lin_reg_cln = LinearRegression()\n",
    "lin_reg_cln.fit(X_train_iqr, y_train_iqr)\n",
    "print(f'Training R^2: {lin_reg_cln.score(X_train_iqr, y_train_iqr):.3f}')\n",
    "print(f'Test R^2: {lin_reg_cln.score(X_test_df, y_test):.3f}')\n",
    "sf.evaluate_ols(lin_reg_cln, X_train_iqr, y_train_iqr)\n",
    "sf.evaluate_ols(lin_reg_cln, X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f449d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## fitting a linear regression model\n",
    "# lin_reg_cln = LinearRegression()\n",
    "# lin_reg_cln.fit(X_train_cln, y_train_cln)\n",
    "# print(f'Training R^2: {lin_reg_cln.score(X_train_cln, y_train_cln):.3f}')\n",
    "# print(f'Test R^2: {lin_reg_cln.score(X_test_df, y_test):.3f}')\n",
    "# sf.evaluate_ols(lin_reg_cln, X_train_cln, y_train_cln)\n",
    "# sf.evaluate_ols(lin_reg_cln, X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09c0ae",
   "metadata": {},
   "source": [
    "## Testing if Zipcode Good for Rare Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe8da7",
   "metadata": {},
   "source": [
    "#### DELETE ME - Testing If zipcode would work for demo'ing rare label encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e25834",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE ME - Testing If zipcode would work for demo'ing rare label encoding\n",
    "zip_counts = df['zipcode'].value_counts(1)\n",
    "zip_counts#.cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE ME\n",
    "zip_counts[zip_counts<.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE ME\n",
    "sns.histplot(zip_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957e11e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DELETE ME - Testing If zipcode would work for demo'ing rare label encoding\n",
    "zip_counts.plot(kind='barh',figsize=(8,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f4aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.439px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
